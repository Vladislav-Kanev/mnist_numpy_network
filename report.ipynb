{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "\n",
    "mndata = MNIST('samples')\n",
    "\n",
    "images, labels = mndata.load_training()\n",
    "\n",
    "test_images, test_labels = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(mndata.train_images)\n",
    "y_train = np.eye(len(np.unique(labels)))[mndata.train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def softmax_derivative(softmax_output, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    grad = softmax_output - y_true\n",
    "    grad /= m\n",
    "    return grad\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def forward(x, w1, b1, w2, b2):\n",
    "    z1 = np.dot(x, w1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = softmax(z2)\n",
    "    return a1, a2\n",
    "\n",
    "\n",
    "def backward(x, a1, a2, y_true, w2):\n",
    "    m = y_true.shape[0]\n",
    "\n",
    "    dz2 = softmax_derivative(a2, y_true)\n",
    "    dw2 = np.dot(a1.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    dz1 = np.dot(dz2, w2.T) * relu_derivative(a1)\n",
    "    dw1 = np.dot(x.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    return dw1, db1, dw2, db2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def predict(x, w1, b1, w2, b2):\n",
    "    _, a2 = forward(x, w1, b1, w2, b2)\n",
    "    return np.argmax(a2, axis=1)\n",
    "\n",
    "def plateau_scheduler(initial_lr, epoch, patience=10, factor=0.5, threshold=1e-4):\n",
    "    return initial_lr * factor if epoch % patience == 0 and threshold < 0 else initial_lr * factor if epoch % patience == 0 else initial_lr\n",
    "\n",
    "def train(x_train, y_train, initial_learning_rate, epochs, batch_size, patience=10, factor=0.5, threshold=1e-4):\n",
    "    input_size = x_train.shape[1]\n",
    "    output_size = y_train.shape[1]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    w1 = np.random.randn(input_size, 64) * 0.01\n",
    "    b1 = np.zeros((1, 64))\n",
    "    w2 = np.random.randn(64, output_size) * 0.01\n",
    "    b2 = np.zeros((1, output_size))\n",
    "\n",
    "    m = x_train.shape[0]\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, m, batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "            a1, a2 = forward(x_batch, w1, b1, w2, b2)\n",
    "\n",
    "            loss = mse_loss(y_batch, a2)\n",
    "\n",
    "            dw1, db1, dw2, db2 = backward(x_batch, a1, a2, y_batch, w2)\n",
    "\n",
    "            w1 -= initial_learning_rate * dw1\n",
    "            b1 -= initial_learning_rate * db1\n",
    "            w2 -= initial_learning_rate * dw2\n",
    "            b2 -= initial_learning_rate * db2\n",
    "\n",
    "        print(f'Epoch {epoch}, Loss: {loss}, Learning Rate: {initial_learning_rate}')\n",
    "\n",
    "        if loss < best_loss - threshold:\n",
    "            best_loss = loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                initial_learning_rate = plateau_scheduler(initial_learning_rate, epoch, patience, factor, threshold)\n",
    "                counter = 0\n",
    "\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "epochs = 1000\n",
    "batch_size = 64\n",
    "patience = 10\n",
    "factor = 0.5\n",
    "threshold = 1e-4\n",
    "\n",
    "w1, b1, w2, b2 = train(x_train, y_train, initial_learning_rate, epochs, batch_size, patience, factor, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell output was cleared and posted in separate file `trainingProcess.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(mndata.test_images, w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5826"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred == mndata.test_labels) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
