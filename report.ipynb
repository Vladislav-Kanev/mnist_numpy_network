{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "\n",
    "mndata = MNIST('samples')\n",
    "\n",
    "images, labels = mndata.load_training()\n",
    "\n",
    "test_images, test_labels = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(mndata.train_images)\n",
    "y_train = np.eye(len(np.unique(labels)))[mndata.train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def softmax_derivative(softmax_output, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    grad = softmax_output - y_true\n",
    "    grad /= m\n",
    "    return grad\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def forward(x, w1, b1, w2, b2):\n",
    "    z1 = np.dot(x, w1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = softmax(z2)\n",
    "    return a1, a2\n",
    "\n",
    "\n",
    "def backward(x, a1, a2, y_true, w2):\n",
    "    m = y_true.shape[0]\n",
    "\n",
    "    dz2 = softmax_derivative(a2, y_true)\n",
    "    dw2 = np.dot(a1.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    dz1 = np.dot(dz2, w2.T) * relu_derivative(a1)\n",
    "    dw1 = np.dot(x.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    return dw1, db1, dw2, db2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def predict(x, w1, b1, w2, b2):\n",
    "    _, a2 = forward(x, w1, b1, w2, b2)\n",
    "    return np.argmax(a2, axis=1)\n",
    "\n",
    "def plateau_scheduler(initial_lr, epoch, patience=10, factor=0.5, threshold=1e-4):\n",
    "    return initial_lr * factor if epoch % patience == 0 and threshold < 0 else initial_lr * factor if epoch % patience == 0 else initial_lr\n",
    "\n",
    "def train(x_train, y_train, initial_learning_rate, epochs, batch_size, patience=10, factor=0.5, threshold=1e-4):\n",
    "    input_size = x_train.shape[1]\n",
    "    output_size = y_train.shape[1]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    w1 = np.random.randn(input_size, 64) * 0.01\n",
    "    b1 = np.zeros((1, 64))\n",
    "    w2 = np.random.randn(64, output_size) * 0.01\n",
    "    b2 = np.zeros((1, output_size))\n",
    "\n",
    "    m = x_train.shape[0]\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, m, batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "            a1, a2 = forward(x_batch, w1, b1, w2, b2)\n",
    "\n",
    "            loss = mse_loss(y_batch, a2)\n",
    "\n",
    "            dw1, db1, dw2, db2 = backward(x_batch, a1, a2, y_batch, w2)\n",
    "\n",
    "            w1 -= initial_learning_rate * dw1\n",
    "            b1 -= initial_learning_rate * db1\n",
    "            w2 -= initial_learning_rate * dw2\n",
    "            b2 -= initial_learning_rate * db2\n",
    "\n",
    "        print(f'Epoch {epoch}, Loss: {loss}, Learning Rate: {initial_learning_rate}')\n",
    "\n",
    "        if loss < best_loss - threshold:\n",
    "            best_loss = loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                initial_learning_rate = plateau_scheduler(initial_learning_rate, epoch, patience, factor, threshold)\n",
    "                counter = 0\n",
    "\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.07727649052987623, Learning Rate: 0.01\n",
      "Epoch 1, Loss: 0.08275314307235351, Learning Rate: 0.01\n",
      "Epoch 2, Loss: 0.08661588041798153, Learning Rate: 0.01\n",
      "Epoch 3, Loss: 0.08315959309535957, Learning Rate: 0.01\n",
      "Epoch 4, Loss: 0.0821478917413839, Learning Rate: 0.01\n",
      "Epoch 5, Loss: 0.08667872942744423, Learning Rate: 0.01\n",
      "Epoch 6, Loss: 0.08009472888765987, Learning Rate: 0.01\n",
      "Epoch 7, Loss: 0.09003947285049756, Learning Rate: 0.01\n",
      "Epoch 8, Loss: 0.08051656273885399, Learning Rate: 0.01\n",
      "Epoch 9, Loss: 0.08058031955489649, Learning Rate: 0.01\n",
      "Epoch 10, Loss: 0.0805993097500218, Learning Rate: 0.01\n",
      "Epoch 11, Loss: 0.0806053072907421, Learning Rate: 0.005\n",
      "Epoch 12, Loss: 0.08060680182652646, Learning Rate: 0.005\n",
      "Epoch 13, Loss: 0.08060827626294406, Learning Rate: 0.005\n",
      "Epoch 14, Loss: 0.08060921286678216, Learning Rate: 0.005\n",
      "Epoch 15, Loss: 0.08060941297740336, Learning Rate: 0.005\n",
      "Epoch 16, Loss: 0.08060788805704393, Learning Rate: 0.005\n",
      "Epoch 17, Loss: 0.08060718669503118, Learning Rate: 0.005\n",
      "Epoch 18, Loss: 0.08060711395694868, Learning Rate: 0.005\n",
      "Epoch 19, Loss: 0.08060763322335511, Learning Rate: 0.005\n",
      "Epoch 20, Loss: 0.08060847727933576, Learning Rate: 0.005\n",
      "Epoch 21, Loss: 0.08061035302018199, Learning Rate: 0.0025\n",
      "Epoch 22, Loss: 0.08061169546871767, Learning Rate: 0.0025\n",
      "Epoch 23, Loss: 0.08061314011984017, Learning Rate: 0.0025\n",
      "Epoch 24, Loss: 0.08061434248784587, Learning Rate: 0.0025\n",
      "Epoch 25, Loss: 0.08001319287932751, Learning Rate: 0.0025\n",
      "Epoch 26, Loss: 0.08061403883891491, Learning Rate: 0.0025\n",
      "Epoch 27, Loss: 0.08061389008225228, Learning Rate: 0.0025\n",
      "Epoch 28, Loss: 0.08055143595338735, Learning Rate: 0.0025\n",
      "Epoch 29, Loss: 0.07941856920764791, Learning Rate: 0.0025\n",
      "Epoch 30, Loss: 0.07907392258510591, Learning Rate: 0.0025\n",
      "Epoch 31, Loss: 0.07817425020027524, Learning Rate: 0.00125\n",
      "Epoch 32, Loss: 0.07742555384235679, Learning Rate: 0.00125\n",
      "Epoch 33, Loss: 0.0771270398364067, Learning Rate: 0.00125\n",
      "Epoch 34, Loss: 0.07716712057983129, Learning Rate: 0.00125\n",
      "Epoch 35, Loss: 0.077352764652883, Learning Rate: 0.00125\n",
      "Epoch 36, Loss: 0.07767615037321111, Learning Rate: 0.00125\n",
      "Epoch 37, Loss: 0.07790953696923156, Learning Rate: 0.00125\n",
      "Epoch 38, Loss: 0.07796111249404444, Learning Rate: 0.00125\n",
      "Epoch 39, Loss: 0.07821534819876487, Learning Rate: 0.00125\n",
      "Epoch 40, Loss: 0.07827604661193228, Learning Rate: 0.00125\n",
      "Epoch 41, Loss: 0.07833127845973915, Learning Rate: 0.00125\n",
      "Epoch 42, Loss: 0.07842980024429191, Learning Rate: 0.00125\n",
      "Epoch 43, Loss: 0.07849888144378815, Learning Rate: 0.00125\n",
      "Epoch 44, Loss: 0.07866837942988743, Learning Rate: 0.00125\n",
      "Epoch 45, Loss: 0.07855527121836606, Learning Rate: 0.00125\n",
      "Epoch 46, Loss: 0.07845645070165894, Learning Rate: 0.00125\n",
      "Epoch 47, Loss: 0.07859722101763969, Learning Rate: 0.00125\n",
      "Epoch 48, Loss: 0.07863075830822418, Learning Rate: 0.00125\n",
      "Epoch 49, Loss: 0.07855716105290572, Learning Rate: 0.00125\n",
      "Epoch 50, Loss: 0.07840098971307718, Learning Rate: 0.00125\n",
      "Epoch 51, Loss: 0.07850480114523613, Learning Rate: 0.00125\n",
      "Epoch 52, Loss: 0.07796724573494769, Learning Rate: 0.00125\n",
      "Epoch 53, Loss: 0.07816852477110206, Learning Rate: 0.00125\n",
      "Epoch 54, Loss: 0.07838731176552792, Learning Rate: 0.00125\n",
      "Epoch 55, Loss: 0.07757342890424322, Learning Rate: 0.00125\n",
      "Epoch 56, Loss: 0.0774751330142385, Learning Rate: 0.00125\n",
      "Epoch 57, Loss: 0.07736186399649753, Learning Rate: 0.00125\n",
      "Epoch 58, Loss: 0.07705611005787891, Learning Rate: 0.00125\n",
      "Epoch 59, Loss: 0.07697329845345872, Learning Rate: 0.00125\n",
      "Epoch 60, Loss: 0.07701487030356856, Learning Rate: 0.00125\n",
      "Epoch 61, Loss: 0.07723846713146518, Learning Rate: 0.00125\n",
      "Epoch 62, Loss: 0.07698579896925342, Learning Rate: 0.00125\n",
      "Epoch 63, Loss: 0.07692378171069904, Learning Rate: 0.00125\n",
      "Epoch 64, Loss: 0.07678556675500209, Learning Rate: 0.00125\n",
      "Epoch 65, Loss: 0.07206733215570266, Learning Rate: 0.00125\n",
      "Epoch 66, Loss: 0.06677005190753209, Learning Rate: 0.00125\n",
      "Epoch 67, Loss: 0.0660828274879526, Learning Rate: 0.00125\n",
      "Epoch 68, Loss: 0.06534346316316811, Learning Rate: 0.00125\n",
      "Epoch 69, Loss: 0.06525751886359651, Learning Rate: 0.00125\n",
      "Epoch 70, Loss: 0.06514101934549975, Learning Rate: 0.00125\n",
      "Epoch 71, Loss: 0.06494266996834526, Learning Rate: 0.00125\n",
      "Epoch 72, Loss: 0.06429272718608496, Learning Rate: 0.00125\n",
      "Epoch 73, Loss: 0.06432368366176142, Learning Rate: 0.00125\n",
      "Epoch 74, Loss: 0.0641639845697621, Learning Rate: 0.00125\n",
      "Epoch 75, Loss: 0.06390871239223495, Learning Rate: 0.00125\n",
      "Epoch 76, Loss: 0.06391415871095915, Learning Rate: 0.00125\n",
      "Epoch 77, Loss: 0.06436398614416325, Learning Rate: 0.00125\n",
      "Epoch 78, Loss: 0.0651985404504096, Learning Rate: 0.00125\n",
      "Epoch 79, Loss: 0.06522923790125397, Learning Rate: 0.00125\n",
      "Epoch 80, Loss: 0.06569403238365987, Learning Rate: 0.00125\n",
      "Epoch 81, Loss: 0.06559564828833417, Learning Rate: 0.00125\n",
      "Epoch 82, Loss: 0.06535562653767817, Learning Rate: 0.00125\n",
      "Epoch 83, Loss: 0.06562377876811012, Learning Rate: 0.00125\n",
      "Epoch 84, Loss: 0.06540064801373682, Learning Rate: 0.00125\n",
      "Epoch 85, Loss: 0.06574623001640309, Learning Rate: 0.00125\n",
      "Epoch 86, Loss: 0.06513609679808137, Learning Rate: 0.00125\n",
      "Epoch 87, Loss: 0.06502471022145773, Learning Rate: 0.00125\n",
      "Epoch 88, Loss: 0.06500073966459279, Learning Rate: 0.00125\n",
      "Epoch 89, Loss: 0.06504688070754039, Learning Rate: 0.00125\n",
      "Epoch 90, Loss: 0.06412267655647182, Learning Rate: 0.00125\n",
      "Epoch 91, Loss: 0.06504278520597954, Learning Rate: 0.00125\n",
      "Epoch 92, Loss: 0.06544424601436695, Learning Rate: 0.00125\n",
      "Epoch 93, Loss: 0.06493633008147115, Learning Rate: 0.00125\n",
      "Epoch 94, Loss: 0.06481161243839599, Learning Rate: 0.00125\n",
      "Epoch 95, Loss: 0.0647394366389496, Learning Rate: 0.00125\n",
      "Epoch 96, Loss: 0.06499784409068358, Learning Rate: 0.00125\n",
      "Epoch 97, Loss: 0.06460499700674727, Learning Rate: 0.00125\n",
      "Epoch 98, Loss: 0.06483728586807427, Learning Rate: 0.00125\n",
      "Epoch 99, Loss: 0.06486988888402868, Learning Rate: 0.00125\n",
      "Epoch 100, Loss: 0.06430674033466025, Learning Rate: 0.00125\n",
      "Epoch 101, Loss: 0.06446348907259272, Learning Rate: 0.00125\n",
      "Epoch 102, Loss: 0.06393153021191778, Learning Rate: 0.00125\n",
      "Epoch 103, Loss: 0.06464575323055785, Learning Rate: 0.00125\n",
      "Epoch 104, Loss: 0.06409869231029616, Learning Rate: 0.00125\n",
      "Epoch 105, Loss: 0.0640651055605933, Learning Rate: 0.00125\n",
      "Epoch 106, Loss: 0.06394018008584376, Learning Rate: 0.00125\n",
      "Epoch 107, Loss: 0.06398302554621195, Learning Rate: 0.00125\n",
      "Epoch 108, Loss: 0.06389515187839914, Learning Rate: 0.00125\n",
      "Epoch 109, Loss: 0.06427015063726625, Learning Rate: 0.00125\n",
      "Epoch 110, Loss: 0.06380706231165875, Learning Rate: 0.00125\n",
      "Epoch 111, Loss: 0.06363298433195955, Learning Rate: 0.00125\n",
      "Epoch 112, Loss: 0.06376186065697138, Learning Rate: 0.00125\n",
      "Epoch 113, Loss: 0.06389957009710945, Learning Rate: 0.00125\n",
      "Epoch 114, Loss: 0.06363864571453523, Learning Rate: 0.00125\n",
      "Epoch 115, Loss: 0.06408365321581296, Learning Rate: 0.00125\n",
      "Epoch 116, Loss: 0.06392351433742613, Learning Rate: 0.00125\n",
      "Epoch 117, Loss: 0.06396706509982482, Learning Rate: 0.00125\n",
      "Epoch 118, Loss: 0.06418751062181431, Learning Rate: 0.00125\n",
      "Epoch 119, Loss: 0.06400180339575189, Learning Rate: 0.00125\n",
      "Epoch 120, Loss: 0.06359663996150385, Learning Rate: 0.00125\n",
      "Epoch 121, Loss: 0.06374541271435061, Learning Rate: 0.00125\n",
      "Epoch 122, Loss: 0.0639202112310151, Learning Rate: 0.00125\n",
      "Epoch 123, Loss: 0.06379400753965371, Learning Rate: 0.00125\n",
      "Epoch 124, Loss: 0.06384282919673814, Learning Rate: 0.00125\n",
      "Epoch 125, Loss: 0.06382091579760925, Learning Rate: 0.00125\n",
      "Epoch 126, Loss: 0.0640091457166064, Learning Rate: 0.00125\n",
      "Epoch 127, Loss: 0.06297858021892067, Learning Rate: 0.00125\n",
      "Epoch 128, Loss: 0.06297380333503466, Learning Rate: 0.00125\n",
      "Epoch 129, Loss: 0.06315790318165071, Learning Rate: 0.00125\n",
      "Epoch 130, Loss: 0.0635986977361426, Learning Rate: 0.00125\n",
      "Epoch 131, Loss: 0.0635265880679646, Learning Rate: 0.00125\n",
      "Epoch 132, Loss: 0.06359420795064359, Learning Rate: 0.00125\n",
      "Epoch 133, Loss: 0.06310312466730453, Learning Rate: 0.00125\n",
      "Epoch 134, Loss: 0.06350497864883184, Learning Rate: 0.00125\n",
      "Epoch 135, Loss: 0.06363294071395142, Learning Rate: 0.00125\n",
      "Epoch 136, Loss: 0.06370225210873126, Learning Rate: 0.00125\n",
      "Epoch 137, Loss: 0.06402048180761995, Learning Rate: 0.00125\n",
      "Epoch 138, Loss: 0.06379767861326521, Learning Rate: 0.00125\n",
      "Epoch 139, Loss: 0.06404264913600174, Learning Rate: 0.00125\n",
      "Epoch 140, Loss: 0.06404601631416715, Learning Rate: 0.00125\n",
      "Epoch 141, Loss: 0.0638531993732776, Learning Rate: 0.00125\n",
      "Epoch 142, Loss: 0.06379436931373265, Learning Rate: 0.00125\n",
      "Epoch 143, Loss: 0.0641614628055871, Learning Rate: 0.00125\n",
      "Epoch 144, Loss: 0.06425385233102815, Learning Rate: 0.00125\n",
      "Epoch 145, Loss: 0.06418538563868068, Learning Rate: 0.00125\n",
      "Epoch 146, Loss: 0.06421924544455038, Learning Rate: 0.00125\n",
      "Epoch 147, Loss: 0.06417047332087789, Learning Rate: 0.00125\n",
      "Epoch 148, Loss: 0.06429080045015352, Learning Rate: 0.00125\n",
      "Epoch 149, Loss: 0.0644253095410899, Learning Rate: 0.00125\n",
      "Epoch 150, Loss: 0.06457468203748055, Learning Rate: 0.00125\n",
      "Epoch 151, Loss: 0.06467251327216368, Learning Rate: 0.00125\n",
      "Epoch 152, Loss: 0.06464524483634113, Learning Rate: 0.00125\n",
      "Epoch 153, Loss: 0.06447514233841858, Learning Rate: 0.00125\n",
      "Epoch 154, Loss: 0.0643773196156372, Learning Rate: 0.00125\n",
      "Epoch 155, Loss: 0.06465115581867158, Learning Rate: 0.00125\n",
      "Epoch 156, Loss: 0.0646574676767977, Learning Rate: 0.00125\n",
      "Epoch 157, Loss: 0.06456287555857003, Learning Rate: 0.00125\n",
      "Epoch 158, Loss: 0.06454381459285882, Learning Rate: 0.00125\n",
      "Epoch 159, Loss: 0.06455039208836273, Learning Rate: 0.00125\n",
      "Epoch 160, Loss: 0.06448422225473283, Learning Rate: 0.00125\n",
      "Epoch 161, Loss: 0.06398472130584673, Learning Rate: 0.00125\n",
      "Epoch 162, Loss: 0.06378608522186499, Learning Rate: 0.00125\n",
      "Epoch 163, Loss: 0.06427289133614444, Learning Rate: 0.00125\n",
      "Epoch 164, Loss: 0.06388004284848756, Learning Rate: 0.00125\n",
      "Epoch 165, Loss: 0.06406373458268488, Learning Rate: 0.00125\n",
      "Epoch 166, Loss: 0.06406596498873103, Learning Rate: 0.00125\n",
      "Epoch 167, Loss: 0.06414903992800511, Learning Rate: 0.00125\n",
      "Epoch 168, Loss: 0.06452129858352802, Learning Rate: 0.00125\n",
      "Epoch 169, Loss: 0.06412062192458341, Learning Rate: 0.00125\n",
      "Epoch 170, Loss: 0.06431841644137108, Learning Rate: 0.00125\n",
      "Epoch 171, Loss: 0.06451638637134666, Learning Rate: 0.00125\n",
      "Epoch 172, Loss: 0.06455601935309421, Learning Rate: 0.00125\n",
      "Epoch 173, Loss: 0.06439836251480438, Learning Rate: 0.00125\n",
      "Epoch 174, Loss: 0.06421055123924087, Learning Rate: 0.00125\n",
      "Epoch 175, Loss: 0.06409291618553095, Learning Rate: 0.00125\n",
      "Epoch 176, Loss: 0.06412354163766619, Learning Rate: 0.00125\n",
      "Epoch 177, Loss: 0.06445352878698782, Learning Rate: 0.00125\n",
      "Epoch 178, Loss: 0.06481307463070354, Learning Rate: 0.00125\n",
      "Epoch 179, Loss: 0.06392743630573608, Learning Rate: 0.00125\n",
      "Epoch 180, Loss: 0.06374041465637337, Learning Rate: 0.00125\n",
      "Epoch 181, Loss: 0.06403390959337833, Learning Rate: 0.00125\n",
      "Epoch 182, Loss: 0.06356403489154805, Learning Rate: 0.00125\n",
      "Epoch 183, Loss: 0.063815793303351, Learning Rate: 0.00125\n",
      "Epoch 184, Loss: 0.06365945065085905, Learning Rate: 0.00125\n",
      "Epoch 185, Loss: 0.06367839860763705, Learning Rate: 0.00125\n",
      "Epoch 186, Loss: 0.0635181393488139, Learning Rate: 0.00125\n",
      "Epoch 187, Loss: 0.06343892765942075, Learning Rate: 0.00125\n",
      "Epoch 188, Loss: 0.06365200350753664, Learning Rate: 0.00125\n",
      "Epoch 189, Loss: 0.06343324692125028, Learning Rate: 0.00125\n",
      "Epoch 190, Loss: 0.06365134116806678, Learning Rate: 0.00125\n",
      "Epoch 191, Loss: 0.06352547104277831, Learning Rate: 0.00125\n",
      "Epoch 192, Loss: 0.06349451055816895, Learning Rate: 0.00125\n",
      "Epoch 193, Loss: 0.06337108978041901, Learning Rate: 0.00125\n",
      "Epoch 194, Loss: 0.06342265016628709, Learning Rate: 0.00125\n",
      "Epoch 195, Loss: 0.06374829970572667, Learning Rate: 0.00125\n",
      "Epoch 196, Loss: 0.06344356858300385, Learning Rate: 0.00125\n",
      "Epoch 197, Loss: 0.06368280022423206, Learning Rate: 0.00125\n",
      "Epoch 198, Loss: 0.06314138814993273, Learning Rate: 0.00125\n",
      "Epoch 199, Loss: 0.06319703759919834, Learning Rate: 0.00125\n",
      "Epoch 200, Loss: 0.06370848366602821, Learning Rate: 0.00125\n",
      "Epoch 201, Loss: 0.06357695083584673, Learning Rate: 0.00125\n",
      "Epoch 202, Loss: 0.06348098300394331, Learning Rate: 0.00125\n",
      "Epoch 203, Loss: 0.06308751650121518, Learning Rate: 0.00125\n",
      "Epoch 204, Loss: 0.0632643767248183, Learning Rate: 0.00125\n",
      "Epoch 205, Loss: 0.06294276286312044, Learning Rate: 0.00125\n",
      "Epoch 206, Loss: 0.06293176606268043, Learning Rate: 0.00125\n",
      "Epoch 207, Loss: 0.06332397022972946, Learning Rate: 0.00125\n",
      "Epoch 208, Loss: 0.06436258444652121, Learning Rate: 0.00125\n",
      "Epoch 209, Loss: 0.06429944787371886, Learning Rate: 0.00125\n",
      "Epoch 210, Loss: 0.06445151324189487, Learning Rate: 0.00125\n",
      "Epoch 211, Loss: 0.06341732019191257, Learning Rate: 0.00125\n",
      "Epoch 212, Loss: 0.06372713341005451, Learning Rate: 0.00125\n",
      "Epoch 213, Loss: 0.06392756730181302, Learning Rate: 0.00125\n",
      "Epoch 214, Loss: 0.06409895745843606, Learning Rate: 0.00125\n",
      "Epoch 215, Loss: 0.06377475547408175, Learning Rate: 0.00125\n",
      "Epoch 216, Loss: 0.06290383339478103, Learning Rate: 0.00125\n",
      "Epoch 217, Loss: 0.0629108028269747, Learning Rate: 0.00125\n",
      "Epoch 218, Loss: 0.0627607336764892, Learning Rate: 0.00125\n",
      "Epoch 219, Loss: 0.06275972289080181, Learning Rate: 0.00125\n",
      "Epoch 220, Loss: 0.06286022368750835, Learning Rate: 0.00125\n",
      "Epoch 221, Loss: 0.06253676307508046, Learning Rate: 0.00125\n",
      "Epoch 222, Loss: 0.06255840148867761, Learning Rate: 0.00125\n",
      "Epoch 223, Loss: 0.06279553574444255, Learning Rate: 0.00125\n",
      "Epoch 224, Loss: 0.06272254156795429, Learning Rate: 0.00125\n",
      "Epoch 225, Loss: 0.06275072267670283, Learning Rate: 0.00125\n",
      "Epoch 226, Loss: 0.06330403399077784, Learning Rate: 0.00125\n",
      "Epoch 227, Loss: 0.062493803442550136, Learning Rate: 0.00125\n",
      "Epoch 228, Loss: 0.06391379581124372, Learning Rate: 0.00125\n",
      "Epoch 229, Loss: 0.06263546908577727, Learning Rate: 0.00125\n",
      "Epoch 230, Loss: 0.06271565857657545, Learning Rate: 0.00125\n",
      "Epoch 231, Loss: 0.06403279873895404, Learning Rate: 0.00125\n",
      "Epoch 232, Loss: 0.06341960024182311, Learning Rate: 0.00125\n",
      "Epoch 233, Loss: 0.06356259808298217, Learning Rate: 0.00125\n",
      "Epoch 234, Loss: 0.0628435273595682, Learning Rate: 0.00125\n",
      "Epoch 235, Loss: 0.06422881507253758, Learning Rate: 0.00125\n",
      "Epoch 236, Loss: 0.06399616397327972, Learning Rate: 0.00125\n",
      "Epoch 237, Loss: 0.06398553428717865, Learning Rate: 0.00125\n",
      "Epoch 238, Loss: 0.06391287377096219, Learning Rate: 0.00125\n",
      "Epoch 239, Loss: 0.06396696554181935, Learning Rate: 0.00125\n",
      "Epoch 240, Loss: 0.06389924080268343, Learning Rate: 0.00125\n",
      "Epoch 241, Loss: 0.06405611557954982, Learning Rate: 0.00125\n",
      "Epoch 242, Loss: 0.06400423504666376, Learning Rate: 0.00125\n",
      "Epoch 243, Loss: 0.06402232948765954, Learning Rate: 0.00125\n",
      "Epoch 244, Loss: 0.06390373181234665, Learning Rate: 0.00125\n",
      "Epoch 245, Loss: 0.06390608127534816, Learning Rate: 0.00125\n",
      "Epoch 246, Loss: 0.0638561742225778, Learning Rate: 0.00125\n",
      "Epoch 247, Loss: 0.06363593081065921, Learning Rate: 0.00125\n",
      "Epoch 248, Loss: 0.06285545879261642, Learning Rate: 0.00125\n",
      "Epoch 249, Loss: 0.06285756007488712, Learning Rate: 0.00125\n",
      "Epoch 250, Loss: 0.06269663439233325, Learning Rate: 0.00125\n",
      "Epoch 251, Loss: 0.06269141840031767, Learning Rate: 0.00125\n",
      "Epoch 252, Loss: 0.06267832120856195, Learning Rate: 0.00125\n",
      "Epoch 253, Loss: 0.06276988160421641, Learning Rate: 0.00125\n",
      "Epoch 254, Loss: 0.06279840694468074, Learning Rate: 0.00125\n",
      "Epoch 255, Loss: 0.0627169548109382, Learning Rate: 0.00125\n",
      "Epoch 256, Loss: 0.06263008160626918, Learning Rate: 0.00125\n",
      "Epoch 257, Loss: 0.06269886656426119, Learning Rate: 0.00125\n",
      "Epoch 258, Loss: 0.06259444687272073, Learning Rate: 0.00125\n",
      "Epoch 259, Loss: 0.0625414836103921, Learning Rate: 0.00125\n",
      "Epoch 260, Loss: 0.06265832873619778, Learning Rate: 0.00125\n",
      "Epoch 261, Loss: 0.06311954157511314, Learning Rate: 0.00125\n",
      "Epoch 262, Loss: 0.06291573505075446, Learning Rate: 0.00125\n",
      "Epoch 263, Loss: 0.06286546910907545, Learning Rate: 0.00125\n",
      "Epoch 264, Loss: 0.06228216173862455, Learning Rate: 0.00125\n",
      "Epoch 265, Loss: 0.06281227849059387, Learning Rate: 0.00125\n",
      "Epoch 266, Loss: 0.0627944756683938, Learning Rate: 0.00125\n",
      "Epoch 267, Loss: 0.06171539644374093, Learning Rate: 0.00125\n",
      "Epoch 268, Loss: 0.06211062447969862, Learning Rate: 0.00125\n",
      "Epoch 269, Loss: 0.06205622429500028, Learning Rate: 0.00125\n",
      "Epoch 270, Loss: 0.061792272744285184, Learning Rate: 0.00125\n",
      "Epoch 271, Loss: 0.06291961365847733, Learning Rate: 0.00125\n",
      "Epoch 272, Loss: 0.06216195892051283, Learning Rate: 0.00125\n",
      "Epoch 273, Loss: 0.06228640059219581, Learning Rate: 0.00125\n",
      "Epoch 274, Loss: 0.0620670311146033, Learning Rate: 0.00125\n",
      "Epoch 275, Loss: 0.06199278494344207, Learning Rate: 0.00125\n",
      "Epoch 276, Loss: 0.062132322356692714, Learning Rate: 0.00125\n",
      "Epoch 277, Loss: 0.062090322308888536, Learning Rate: 0.00125\n",
      "Epoch 278, Loss: 0.06189577202368722, Learning Rate: 0.00125\n",
      "Epoch 279, Loss: 0.061816157470427235, Learning Rate: 0.00125\n",
      "Epoch 280, Loss: 0.061780219789210625, Learning Rate: 0.00125\n",
      "Epoch 281, Loss: 0.06192787344991901, Learning Rate: 0.00125\n",
      "Epoch 282, Loss: 0.0610773383924744, Learning Rate: 0.00125\n",
      "Epoch 283, Loss: 0.061055853326530116, Learning Rate: 0.00125\n",
      "Epoch 284, Loss: 0.06153206552103391, Learning Rate: 0.00125\n",
      "Epoch 285, Loss: 0.061113226530485856, Learning Rate: 0.00125\n",
      "Epoch 286, Loss: 0.06153243422896328, Learning Rate: 0.00125\n",
      "Epoch 287, Loss: 0.06211896170287133, Learning Rate: 0.00125\n",
      "Epoch 288, Loss: 0.06174946899992051, Learning Rate: 0.00125\n",
      "Epoch 289, Loss: 0.061918161481055264, Learning Rate: 0.00125\n",
      "Epoch 290, Loss: 0.06109647137727785, Learning Rate: 0.00125\n",
      "Epoch 291, Loss: 0.0613980082885827, Learning Rate: 0.00125\n",
      "Epoch 292, Loss: 0.060471105816380245, Learning Rate: 0.00125\n",
      "Epoch 293, Loss: 0.06086625338742433, Learning Rate: 0.00125\n",
      "Epoch 294, Loss: 0.06104115427501029, Learning Rate: 0.00125\n",
      "Epoch 295, Loss: 0.060789115230063785, Learning Rate: 0.00125\n",
      "Epoch 296, Loss: 0.060780121124846764, Learning Rate: 0.00125\n",
      "Epoch 297, Loss: 0.06051220330123972, Learning Rate: 0.00125\n",
      "Epoch 298, Loss: 0.0625792077608699, Learning Rate: 0.00125\n",
      "Epoch 299, Loss: 0.06238936674608394, Learning Rate: 0.00125\n",
      "Epoch 300, Loss: 0.062221813793451555, Learning Rate: 0.00125\n",
      "Epoch 301, Loss: 0.06156331432612629, Learning Rate: 0.00125\n",
      "Epoch 302, Loss: 0.060169213229675986, Learning Rate: 0.00125\n",
      "Epoch 303, Loss: 0.06200243542996446, Learning Rate: 0.00125\n",
      "Epoch 304, Loss: 0.06151296563524229, Learning Rate: 0.00125\n",
      "Epoch 305, Loss: 0.06030235378259674, Learning Rate: 0.00125\n",
      "Epoch 306, Loss: 0.06022755086364985, Learning Rate: 0.00125\n",
      "Epoch 307, Loss: 0.06018701547180151, Learning Rate: 0.00125\n",
      "Epoch 308, Loss: 0.06151235420602165, Learning Rate: 0.00125\n",
      "Epoch 309, Loss: 0.06000511138503306, Learning Rate: 0.00125\n",
      "Epoch 310, Loss: 0.060004467348110234, Learning Rate: 0.00125\n",
      "Epoch 311, Loss: 0.06008248710681856, Learning Rate: 0.00125\n",
      "Epoch 312, Loss: 0.059843851771444426, Learning Rate: 0.00125\n",
      "Epoch 313, Loss: 0.061096921736371726, Learning Rate: 0.00125\n",
      "Epoch 314, Loss: 0.060938114463085315, Learning Rate: 0.00125\n",
      "Epoch 315, Loss: 0.060818980029435435, Learning Rate: 0.00125\n",
      "Epoch 316, Loss: 0.05957656242376328, Learning Rate: 0.00125\n",
      "Epoch 317, Loss: 0.05966674387270843, Learning Rate: 0.00125\n",
      "Epoch 318, Loss: 0.05937547819809326, Learning Rate: 0.00125\n",
      "Epoch 319, Loss: 0.059957356004539754, Learning Rate: 0.00125\n",
      "Epoch 320, Loss: 0.059668001062946105, Learning Rate: 0.00125\n",
      "Epoch 321, Loss: 0.05956982541187905, Learning Rate: 0.00125\n",
      "Epoch 322, Loss: 0.06069235219167748, Learning Rate: 0.00125\n",
      "Epoch 323, Loss: 0.05967041045737339, Learning Rate: 0.00125\n",
      "Epoch 324, Loss: 0.059549534565594954, Learning Rate: 0.00125\n",
      "Epoch 325, Loss: 0.05915852491686924, Learning Rate: 0.00125\n",
      "Epoch 326, Loss: 0.05913034044086455, Learning Rate: 0.00125\n",
      "Epoch 327, Loss: 0.05946024068659482, Learning Rate: 0.00125\n",
      "Epoch 328, Loss: 0.05912508186626577, Learning Rate: 0.00125\n",
      "Epoch 329, Loss: 0.0591713380980274, Learning Rate: 0.00125\n",
      "Epoch 330, Loss: 0.05897018677721013, Learning Rate: 0.00125\n",
      "Epoch 331, Loss: 0.0593055469712328, Learning Rate: 0.00125\n",
      "Epoch 332, Loss: 0.059239813014287504, Learning Rate: 0.00125\n",
      "Epoch 333, Loss: 0.05914776045839225, Learning Rate: 0.00125\n",
      "Epoch 334, Loss: 0.05977393542913485, Learning Rate: 0.00125\n",
      "Epoch 335, Loss: 0.05973808603782456, Learning Rate: 0.00125\n",
      "Epoch 336, Loss: 0.059716387043350715, Learning Rate: 0.00125\n",
      "Epoch 337, Loss: 0.06009441246295884, Learning Rate: 0.00125\n",
      "Epoch 338, Loss: 0.059992946849846664, Learning Rate: 0.00125\n",
      "Epoch 339, Loss: 0.0597038996695823, Learning Rate: 0.00125\n",
      "Epoch 340, Loss: 0.05992940596162213, Learning Rate: 0.00125\n",
      "Epoch 341, Loss: 0.05614168657035655, Learning Rate: 0.000625\n",
      "Epoch 342, Loss: 0.05628135913401298, Learning Rate: 0.000625\n",
      "Epoch 343, Loss: 0.056223648695773856, Learning Rate: 0.000625\n",
      "Epoch 344, Loss: 0.05617557981162764, Learning Rate: 0.000625\n",
      "Epoch 345, Loss: 0.05579372624329004, Learning Rate: 0.000625\n",
      "Epoch 346, Loss: 0.05517063895499119, Learning Rate: 0.000625\n",
      "Epoch 347, Loss: 0.054582210193439874, Learning Rate: 0.000625\n",
      "Epoch 348, Loss: 0.05448929175865681, Learning Rate: 0.000625\n",
      "Epoch 349, Loss: 0.05433848521316755, Learning Rate: 0.000625\n",
      "Epoch 350, Loss: 0.05427550308737548, Learning Rate: 0.000625\n",
      "Epoch 351, Loss: 0.05424299486533316, Learning Rate: 0.000625\n",
      "Epoch 352, Loss: 0.0540299723507761, Learning Rate: 0.000625\n",
      "Epoch 353, Loss: 0.0540302706445472, Learning Rate: 0.000625\n",
      "Epoch 354, Loss: 0.054002274897125536, Learning Rate: 0.000625\n",
      "Epoch 355, Loss: 0.053940788590408975, Learning Rate: 0.000625\n",
      "Epoch 356, Loss: 0.05382296976958795, Learning Rate: 0.000625\n",
      "Epoch 357, Loss: 0.05381251441028636, Learning Rate: 0.000625\n",
      "Epoch 358, Loss: 0.053787926042328536, Learning Rate: 0.000625\n",
      "Epoch 359, Loss: 0.05371623658391374, Learning Rate: 0.000625\n",
      "Epoch 360, Loss: 0.0536831085219762, Learning Rate: 0.000625\n",
      "Epoch 361, Loss: 0.05368725409566799, Learning Rate: 0.000625\n",
      "Epoch 362, Loss: 0.053645052439923124, Learning Rate: 0.000625\n",
      "Epoch 363, Loss: 0.053617965698501815, Learning Rate: 0.000625\n",
      "Epoch 364, Loss: 0.05358670213232458, Learning Rate: 0.000625\n",
      "Epoch 365, Loss: 0.053552241085829455, Learning Rate: 0.000625\n",
      "Epoch 366, Loss: 0.05357295317931854, Learning Rate: 0.000625\n",
      "Epoch 367, Loss: 0.053515920439988054, Learning Rate: 0.000625\n",
      "Epoch 368, Loss: 0.053512290360321645, Learning Rate: 0.000625\n",
      "Epoch 369, Loss: 0.053471034964249375, Learning Rate: 0.000625\n",
      "Epoch 370, Loss: 0.05345185202643974, Learning Rate: 0.000625\n",
      "Epoch 371, Loss: 0.053438822568207624, Learning Rate: 0.000625\n",
      "Epoch 372, Loss: 0.05356133744517446, Learning Rate: 0.000625\n",
      "Epoch 373, Loss: 0.05357008684192609, Learning Rate: 0.000625\n",
      "Epoch 374, Loss: 0.05364431148843971, Learning Rate: 0.000625\n",
      "Epoch 375, Loss: 0.05374096698069417, Learning Rate: 0.000625\n",
      "Epoch 376, Loss: 0.053779334712027116, Learning Rate: 0.000625\n",
      "Epoch 377, Loss: 0.053794728754067725, Learning Rate: 0.000625\n",
      "Epoch 378, Loss: 0.053823640810045134, Learning Rate: 0.000625\n",
      "Epoch 379, Loss: 0.053869283371903796, Learning Rate: 0.000625\n",
      "Epoch 380, Loss: 0.05387186922934668, Learning Rate: 0.000625\n",
      "Epoch 381, Loss: 0.053912090883659725, Learning Rate: 0.000625\n",
      "Epoch 382, Loss: 0.05410153179110203, Learning Rate: 0.000625\n",
      "Epoch 383, Loss: 0.054157295349000344, Learning Rate: 0.000625\n",
      "Epoch 384, Loss: 0.054264735104066916, Learning Rate: 0.000625\n",
      "Epoch 385, Loss: 0.05429810062961482, Learning Rate: 0.000625\n",
      "Epoch 386, Loss: 0.054325363990617624, Learning Rate: 0.000625\n",
      "Epoch 387, Loss: 0.05434787389778217, Learning Rate: 0.000625\n",
      "Epoch 388, Loss: 0.05437159938153002, Learning Rate: 0.000625\n",
      "Epoch 389, Loss: 0.054379723146346245, Learning Rate: 0.000625\n",
      "Epoch 390, Loss: 0.05437927273032521, Learning Rate: 0.000625\n",
      "Epoch 391, Loss: 0.05438776682570384, Learning Rate: 0.000625\n",
      "Epoch 392, Loss: 0.05436232060862387, Learning Rate: 0.000625\n",
      "Epoch 393, Loss: 0.05433842271833637, Learning Rate: 0.000625\n",
      "Epoch 394, Loss: 0.054361099126231105, Learning Rate: 0.000625\n",
      "Epoch 395, Loss: 0.05439155002405852, Learning Rate: 0.000625\n",
      "Epoch 396, Loss: 0.05438177126994339, Learning Rate: 0.000625\n",
      "Epoch 397, Loss: 0.05435847031726306, Learning Rate: 0.000625\n",
      "Epoch 398, Loss: 0.05445760217540396, Learning Rate: 0.000625\n",
      "Epoch 399, Loss: 0.054439976943640625, Learning Rate: 0.000625\n",
      "Epoch 400, Loss: 0.05442999020022734, Learning Rate: 0.000625\n",
      "Epoch 401, Loss: 0.05445568406912771, Learning Rate: 0.000625\n",
      "Epoch 402, Loss: 0.05434775200404589, Learning Rate: 0.000625\n",
      "Epoch 403, Loss: 0.05438076798975515, Learning Rate: 0.000625\n",
      "Epoch 404, Loss: 0.05435259329703936, Learning Rate: 0.000625\n",
      "Epoch 405, Loss: 0.05442552780536183, Learning Rate: 0.000625\n",
      "Epoch 406, Loss: 0.054500606075389355, Learning Rate: 0.000625\n",
      "Epoch 407, Loss: 0.05454514525876175, Learning Rate: 0.000625\n",
      "Epoch 408, Loss: 0.0545498504837761, Learning Rate: 0.000625\n",
      "Epoch 409, Loss: 0.054498982828245356, Learning Rate: 0.000625\n",
      "Epoch 410, Loss: 0.05450909512101139, Learning Rate: 0.000625\n",
      "Epoch 411, Loss: 0.05448556775542399, Learning Rate: 0.000625\n",
      "Epoch 412, Loss: 0.05452433842520921, Learning Rate: 0.000625\n",
      "Epoch 413, Loss: 0.054571795513486385, Learning Rate: 0.000625\n",
      "Epoch 414, Loss: 0.05448696763885182, Learning Rate: 0.000625\n",
      "Epoch 415, Loss: 0.054447838691124795, Learning Rate: 0.000625\n",
      "Epoch 416, Loss: 0.05443093719291738, Learning Rate: 0.000625\n",
      "Epoch 417, Loss: 0.05438917245296384, Learning Rate: 0.000625\n",
      "Epoch 418, Loss: 0.05440593132531152, Learning Rate: 0.000625\n",
      "Epoch 419, Loss: 0.05438737322488894, Learning Rate: 0.000625\n",
      "Epoch 420, Loss: 0.05442901300505594, Learning Rate: 0.000625\n",
      "Epoch 421, Loss: 0.05442918464648057, Learning Rate: 0.000625\n",
      "Epoch 422, Loss: 0.05440399431799765, Learning Rate: 0.000625\n",
      "Epoch 423, Loss: 0.05439753676834613, Learning Rate: 0.000625\n",
      "Epoch 424, Loss: 0.054403597477475266, Learning Rate: 0.000625\n",
      "Epoch 425, Loss: 0.054440800883085186, Learning Rate: 0.000625\n",
      "Epoch 426, Loss: 0.054467754326456876, Learning Rate: 0.000625\n",
      "Epoch 427, Loss: 0.05448455645681429, Learning Rate: 0.000625\n",
      "Epoch 428, Loss: 0.054526938530215605, Learning Rate: 0.000625\n",
      "Epoch 429, Loss: 0.0545273420234619, Learning Rate: 0.000625\n",
      "Epoch 430, Loss: 0.05458051216761661, Learning Rate: 0.000625\n",
      "Epoch 431, Loss: 0.05468838958516348, Learning Rate: 0.000625\n",
      "Epoch 432, Loss: 0.0547099977365596, Learning Rate: 0.000625\n",
      "Epoch 433, Loss: 0.05475744429105239, Learning Rate: 0.000625\n",
      "Epoch 434, Loss: 0.05474156835216741, Learning Rate: 0.000625\n",
      "Epoch 435, Loss: 0.05477071474121735, Learning Rate: 0.000625\n",
      "Epoch 436, Loss: 0.054769960683149266, Learning Rate: 0.000625\n",
      "Epoch 437, Loss: 0.054770544045015225, Learning Rate: 0.000625\n",
      "Epoch 438, Loss: 0.0547891812447512, Learning Rate: 0.000625\n",
      "Epoch 439, Loss: 0.05482057960232602, Learning Rate: 0.000625\n",
      "Epoch 440, Loss: 0.054842378265567336, Learning Rate: 0.000625\n",
      "Epoch 441, Loss: 0.05478468058419299, Learning Rate: 0.000625\n",
      "Epoch 442, Loss: 0.05469700495648917, Learning Rate: 0.000625\n",
      "Epoch 443, Loss: 0.05461025160505688, Learning Rate: 0.000625\n",
      "Epoch 444, Loss: 0.054566551245098426, Learning Rate: 0.000625\n",
      "Epoch 445, Loss: 0.05458072212168581, Learning Rate: 0.000625\n",
      "Epoch 446, Loss: 0.05443300910185023, Learning Rate: 0.000625\n",
      "Epoch 447, Loss: 0.05445210251974643, Learning Rate: 0.000625\n",
      "Epoch 448, Loss: 0.05429815791981039, Learning Rate: 0.000625\n",
      "Epoch 449, Loss: 0.054127337482658744, Learning Rate: 0.000625\n",
      "Epoch 450, Loss: 0.05398494026958499, Learning Rate: 0.000625\n",
      "Epoch 451, Loss: 0.05387088354377846, Learning Rate: 0.000625\n",
      "Epoch 452, Loss: 0.0539065970817836, Learning Rate: 0.000625\n",
      "Epoch 453, Loss: 0.05380552032350244, Learning Rate: 0.000625\n",
      "Epoch 454, Loss: 0.05366623867975655, Learning Rate: 0.000625\n",
      "Epoch 455, Loss: 0.053554996482761554, Learning Rate: 0.000625\n",
      "Epoch 456, Loss: 0.05366161545154853, Learning Rate: 0.000625\n",
      "Epoch 457, Loss: 0.05349870837820854, Learning Rate: 0.000625\n",
      "Epoch 458, Loss: 0.053387603514375416, Learning Rate: 0.000625\n",
      "Epoch 459, Loss: 0.0531566870924454, Learning Rate: 0.000625\n",
      "Epoch 460, Loss: 0.053025681180639175, Learning Rate: 0.000625\n",
      "Epoch 461, Loss: 0.05295811855336965, Learning Rate: 0.000625\n",
      "Epoch 462, Loss: 0.05280149004481951, Learning Rate: 0.000625\n",
      "Epoch 463, Loss: 0.05257774835934661, Learning Rate: 0.000625\n",
      "Epoch 464, Loss: 0.052535127851776366, Learning Rate: 0.000625\n",
      "Epoch 465, Loss: 0.052337851994029114, Learning Rate: 0.000625\n",
      "Epoch 466, Loss: 0.052200973718443656, Learning Rate: 0.000625\n",
      "Epoch 467, Loss: 0.051888889949395556, Learning Rate: 0.000625\n",
      "Epoch 468, Loss: 0.05177461644609919, Learning Rate: 0.000625\n",
      "Epoch 469, Loss: 0.051718098347797614, Learning Rate: 0.000625\n",
      "Epoch 470, Loss: 0.05164320222297564, Learning Rate: 0.000625\n",
      "Epoch 471, Loss: 0.0516178523691585, Learning Rate: 0.000625\n",
      "Epoch 472, Loss: 0.051589281244087085, Learning Rate: 0.000625\n",
      "Epoch 473, Loss: 0.05191755021762334, Learning Rate: 0.000625\n",
      "Epoch 474, Loss: 0.051441316605033906, Learning Rate: 0.000625\n",
      "Epoch 475, Loss: 0.05141678711022004, Learning Rate: 0.000625\n",
      "Epoch 476, Loss: 0.05131319160509272, Learning Rate: 0.000625\n",
      "Epoch 477, Loss: 0.05140338278354806, Learning Rate: 0.000625\n",
      "Epoch 478, Loss: 0.051318417041169385, Learning Rate: 0.000625\n",
      "Epoch 479, Loss: 0.05134012527361349, Learning Rate: 0.000625\n",
      "Epoch 480, Loss: 0.05143332536578229, Learning Rate: 0.000625\n",
      "Epoch 481, Loss: 0.051426946047976194, Learning Rate: 0.000625\n",
      "Epoch 482, Loss: 0.05136267231270804, Learning Rate: 0.000625\n",
      "Epoch 483, Loss: 0.0512987429645546, Learning Rate: 0.000625\n",
      "Epoch 484, Loss: 0.05123724328859425, Learning Rate: 0.000625\n",
      "Epoch 485, Loss: 0.05115778948149898, Learning Rate: 0.000625\n",
      "Epoch 486, Loss: 0.05096470598903036, Learning Rate: 0.000625\n",
      "Epoch 487, Loss: 0.05088699224245412, Learning Rate: 0.000625\n",
      "Epoch 488, Loss: 0.050757859663570604, Learning Rate: 0.000625\n",
      "Epoch 489, Loss: 0.050662450197367856, Learning Rate: 0.000625\n",
      "Epoch 490, Loss: 0.05075363600608692, Learning Rate: 0.000625\n",
      "Epoch 491, Loss: 0.05081165508940026, Learning Rate: 0.000625\n",
      "Epoch 492, Loss: 0.05073474193887482, Learning Rate: 0.000625\n",
      "Epoch 493, Loss: 0.05068135864827822, Learning Rate: 0.000625\n",
      "Epoch 494, Loss: 0.05057238843388464, Learning Rate: 0.000625\n",
      "Epoch 495, Loss: 0.05050757220271683, Learning Rate: 0.000625\n",
      "Epoch 496, Loss: 0.05062068113855359, Learning Rate: 0.000625\n",
      "Epoch 497, Loss: 0.05060177405915196, Learning Rate: 0.000625\n",
      "Epoch 498, Loss: 0.05049261101421758, Learning Rate: 0.000625\n",
      "Epoch 499, Loss: 0.05065826809472618, Learning Rate: 0.000625\n",
      "Epoch 500, Loss: 0.050532661136450496, Learning Rate: 0.000625\n",
      "Epoch 501, Loss: 0.050531248982608745, Learning Rate: 0.000625\n",
      "Epoch 502, Loss: 0.050560908034356, Learning Rate: 0.000625\n",
      "Epoch 503, Loss: 0.050605410263981465, Learning Rate: 0.000625\n",
      "Epoch 504, Loss: 0.05071534177484181, Learning Rate: 0.000625\n",
      "Epoch 505, Loss: 0.05075868354585824, Learning Rate: 0.000625\n",
      "Epoch 506, Loss: 0.05073754694944939, Learning Rate: 0.000625\n",
      "Epoch 507, Loss: 0.050837913729103824, Learning Rate: 0.000625\n",
      "Epoch 508, Loss: 0.05082671351149518, Learning Rate: 0.000625\n",
      "Epoch 509, Loss: 0.05088326629871034, Learning Rate: 0.000625\n",
      "Epoch 510, Loss: 0.05099446568111878, Learning Rate: 0.000625\n",
      "Epoch 511, Loss: 0.05099864840461098, Learning Rate: 0.000625\n",
      "Epoch 512, Loss: 0.05113176893752225, Learning Rate: 0.000625\n",
      "Epoch 513, Loss: 0.05112160692549121, Learning Rate: 0.000625\n",
      "Epoch 514, Loss: 0.05104294521271968, Learning Rate: 0.000625\n",
      "Epoch 515, Loss: 0.05105687104487131, Learning Rate: 0.000625\n",
      "Epoch 516, Loss: 0.0510092611532418, Learning Rate: 0.000625\n",
      "Epoch 517, Loss: 0.05108281985874162, Learning Rate: 0.000625\n",
      "Epoch 518, Loss: 0.0511288789839998, Learning Rate: 0.000625\n",
      "Epoch 519, Loss: 0.051109434570887756, Learning Rate: 0.000625\n",
      "Epoch 520, Loss: 0.05105282861303066, Learning Rate: 0.000625\n",
      "Epoch 521, Loss: 0.05102423176750319, Learning Rate: 0.000625\n",
      "Epoch 522, Loss: 0.05100997712570334, Learning Rate: 0.000625\n",
      "Epoch 523, Loss: 0.05104684170892467, Learning Rate: 0.000625\n",
      "Epoch 524, Loss: 0.051030627020931196, Learning Rate: 0.000625\n",
      "Epoch 525, Loss: 0.050973921478340754, Learning Rate: 0.000625\n",
      "Epoch 526, Loss: 0.05099340282767676, Learning Rate: 0.000625\n",
      "Epoch 527, Loss: 0.05101065086146015, Learning Rate: 0.000625\n",
      "Epoch 528, Loss: 0.05116187409940175, Learning Rate: 0.000625\n",
      "Epoch 529, Loss: 0.051157512089136804, Learning Rate: 0.000625\n",
      "Epoch 530, Loss: 0.05117183909058691, Learning Rate: 0.000625\n",
      "Epoch 531, Loss: 0.05118499025478982, Learning Rate: 0.000625\n",
      "Epoch 532, Loss: 0.05120837320580399, Learning Rate: 0.000625\n",
      "Epoch 533, Loss: 0.05117361626359296, Learning Rate: 0.000625\n",
      "Epoch 534, Loss: 0.05112961969463705, Learning Rate: 0.000625\n",
      "Epoch 535, Loss: 0.05112025153896103, Learning Rate: 0.000625\n",
      "Epoch 536, Loss: 0.0511506417702007, Learning Rate: 0.000625\n",
      "Epoch 537, Loss: 0.05113923589260187, Learning Rate: 0.000625\n",
      "Epoch 538, Loss: 0.051122114735670895, Learning Rate: 0.000625\n",
      "Epoch 539, Loss: 0.051123374389395806, Learning Rate: 0.000625\n",
      "Epoch 540, Loss: 0.051061302449228504, Learning Rate: 0.000625\n",
      "Epoch 541, Loss: 0.05103528511913695, Learning Rate: 0.000625\n",
      "Epoch 542, Loss: 0.05105743498750252, Learning Rate: 0.000625\n",
      "Epoch 543, Loss: 0.05106864747019153, Learning Rate: 0.000625\n",
      "Epoch 544, Loss: 0.0510800679147565, Learning Rate: 0.000625\n",
      "Epoch 545, Loss: 0.050990506903411635, Learning Rate: 0.000625\n",
      "Epoch 546, Loss: 0.05100373223592769, Learning Rate: 0.000625\n",
      "Epoch 547, Loss: 0.050906880702144816, Learning Rate: 0.000625\n",
      "Epoch 548, Loss: 0.05092503476913213, Learning Rate: 0.000625\n",
      "Epoch 549, Loss: 0.05087951890380875, Learning Rate: 0.000625\n",
      "Epoch 550, Loss: 0.050913514383635096, Learning Rate: 0.000625\n",
      "Epoch 551, Loss: 0.050922622271694105, Learning Rate: 0.000625\n",
      "Epoch 552, Loss: 0.05088431513107099, Learning Rate: 0.000625\n",
      "Epoch 553, Loss: 0.050876216293931485, Learning Rate: 0.000625\n",
      "Epoch 554, Loss: 0.05086313190049794, Learning Rate: 0.000625\n",
      "Epoch 555, Loss: 0.05091359308824939, Learning Rate: 0.000625\n",
      "Epoch 556, Loss: 0.05091439190502476, Learning Rate: 0.000625\n",
      "Epoch 557, Loss: 0.05096422193285012, Learning Rate: 0.000625\n",
      "Epoch 558, Loss: 0.05097312446122028, Learning Rate: 0.000625\n",
      "Epoch 559, Loss: 0.050940530281086874, Learning Rate: 0.000625\n",
      "Epoch 560, Loss: 0.05097096451921548, Learning Rate: 0.000625\n",
      "Epoch 561, Loss: 0.05097483825055567, Learning Rate: 0.000625\n",
      "Epoch 562, Loss: 0.05095764637277668, Learning Rate: 0.000625\n",
      "Epoch 563, Loss: 0.05093952517559519, Learning Rate: 0.000625\n",
      "Epoch 564, Loss: 0.05097080576751044, Learning Rate: 0.000625\n",
      "Epoch 565, Loss: 0.051026136454339156, Learning Rate: 0.000625\n",
      "Epoch 566, Loss: 0.050992909152083155, Learning Rate: 0.000625\n",
      "Epoch 567, Loss: 0.05103036744036652, Learning Rate: 0.000625\n",
      "Epoch 568, Loss: 0.05107977247310447, Learning Rate: 0.000625\n",
      "Epoch 569, Loss: 0.051113876839565385, Learning Rate: 0.000625\n",
      "Epoch 570, Loss: 0.051148505290692134, Learning Rate: 0.000625\n",
      "Epoch 571, Loss: 0.051071811900296915, Learning Rate: 0.000625\n",
      "Epoch 572, Loss: 0.05106438430118017, Learning Rate: 0.000625\n",
      "Epoch 573, Loss: 0.051065214549279446, Learning Rate: 0.000625\n",
      "Epoch 574, Loss: 0.05106415883416363, Learning Rate: 0.000625\n",
      "Epoch 575, Loss: 0.051046645827931704, Learning Rate: 0.000625\n",
      "Epoch 576, Loss: 0.051060978836211655, Learning Rate: 0.000625\n",
      "Epoch 577, Loss: 0.05107245608232554, Learning Rate: 0.000625\n",
      "Epoch 578, Loss: 0.05107578114339108, Learning Rate: 0.000625\n",
      "Epoch 579, Loss: 0.05108673054718149, Learning Rate: 0.000625\n",
      "Epoch 580, Loss: 0.05108815250035046, Learning Rate: 0.000625\n",
      "Epoch 581, Loss: 0.05106483757321637, Learning Rate: 0.000625\n",
      "Epoch 582, Loss: 0.05103389799452966, Learning Rate: 0.000625\n",
      "Epoch 583, Loss: 0.05107158798470668, Learning Rate: 0.000625\n",
      "Epoch 584, Loss: 0.05103227946056743, Learning Rate: 0.000625\n",
      "Epoch 585, Loss: 0.051061295582234434, Learning Rate: 0.000625\n",
      "Epoch 586, Loss: 0.051053079248984404, Learning Rate: 0.000625\n",
      "Epoch 587, Loss: 0.051066991892589855, Learning Rate: 0.000625\n",
      "Epoch 588, Loss: 0.0510938162913878, Learning Rate: 0.000625\n",
      "Epoch 589, Loss: 0.05113077898974481, Learning Rate: 0.000625\n",
      "Epoch 590, Loss: 0.05111033519547712, Learning Rate: 0.000625\n",
      "Epoch 591, Loss: 0.05109052972945662, Learning Rate: 0.000625\n",
      "Epoch 592, Loss: 0.05111220033345679, Learning Rate: 0.000625\n",
      "Epoch 593, Loss: 0.05113407321552384, Learning Rate: 0.000625\n",
      "Epoch 594, Loss: 0.051201392208613236, Learning Rate: 0.000625\n",
      "Epoch 595, Loss: 0.051190619020323, Learning Rate: 0.000625\n",
      "Epoch 596, Loss: 0.0512219223520756, Learning Rate: 0.000625\n",
      "Epoch 597, Loss: 0.051222454077741675, Learning Rate: 0.000625\n",
      "Epoch 598, Loss: 0.051247788084403056, Learning Rate: 0.000625\n",
      "Epoch 599, Loss: 0.05125257496151246, Learning Rate: 0.000625\n",
      "Epoch 600, Loss: 0.05124158251643078, Learning Rate: 0.000625\n",
      "Epoch 601, Loss: 0.0513380910875512, Learning Rate: 0.000625\n",
      "Epoch 602, Loss: 0.051306091443153366, Learning Rate: 0.000625\n",
      "Epoch 603, Loss: 0.05129178116641888, Learning Rate: 0.000625\n",
      "Epoch 604, Loss: 0.05131281030671727, Learning Rate: 0.000625\n",
      "Epoch 605, Loss: 0.05130222397216555, Learning Rate: 0.000625\n",
      "Epoch 606, Loss: 0.051309401020370425, Learning Rate: 0.000625\n",
      "Epoch 607, Loss: 0.05131480069887164, Learning Rate: 0.000625\n",
      "Epoch 608, Loss: 0.05137798647909213, Learning Rate: 0.000625\n",
      "Epoch 609, Loss: 0.0513219057946598, Learning Rate: 0.000625\n",
      "Epoch 610, Loss: 0.05133141528790854, Learning Rate: 0.000625\n",
      "Epoch 611, Loss: 0.051316125157745175, Learning Rate: 0.000625\n",
      "Epoch 612, Loss: 0.05130908593942603, Learning Rate: 0.000625\n",
      "Epoch 613, Loss: 0.05131142108855128, Learning Rate: 0.000625\n",
      "Epoch 614, Loss: 0.05131490938672121, Learning Rate: 0.000625\n",
      "Epoch 615, Loss: 0.05130148090973281, Learning Rate: 0.000625\n",
      "Epoch 616, Loss: 0.0513202564307487, Learning Rate: 0.000625\n",
      "Epoch 617, Loss: 0.05133588314197439, Learning Rate: 0.000625\n",
      "Epoch 618, Loss: 0.05135344874254151, Learning Rate: 0.000625\n",
      "Epoch 619, Loss: 0.051332901343541716, Learning Rate: 0.000625\n",
      "Epoch 620, Loss: 0.05138872737757015, Learning Rate: 0.000625\n",
      "Epoch 621, Loss: 0.05141651636136457, Learning Rate: 0.000625\n",
      "Epoch 622, Loss: 0.05142961506176841, Learning Rate: 0.000625\n",
      "Epoch 623, Loss: 0.051553579071321035, Learning Rate: 0.000625\n",
      "Epoch 624, Loss: 0.051536018149180254, Learning Rate: 0.000625\n",
      "Epoch 625, Loss: 0.05150055290877513, Learning Rate: 0.000625\n",
      "Epoch 626, Loss: 0.051508419399056316, Learning Rate: 0.000625\n",
      "Epoch 627, Loss: 0.051585425789396636, Learning Rate: 0.000625\n",
      "Epoch 628, Loss: 0.051576385564912085, Learning Rate: 0.000625\n",
      "Epoch 629, Loss: 0.0516392674424108, Learning Rate: 0.000625\n",
      "Epoch 630, Loss: 0.051664085866607604, Learning Rate: 0.000625\n",
      "Epoch 631, Loss: 0.05170226014210536, Learning Rate: 0.000625\n",
      "Epoch 632, Loss: 0.05180896006219078, Learning Rate: 0.000625\n",
      "Epoch 633, Loss: 0.05189967648396544, Learning Rate: 0.000625\n",
      "Epoch 634, Loss: 0.05190634301166393, Learning Rate: 0.000625\n",
      "Epoch 635, Loss: 0.0518800297749177, Learning Rate: 0.000625\n",
      "Epoch 636, Loss: 0.051934904444498, Learning Rate: 0.000625\n",
      "Epoch 637, Loss: 0.051791243039687705, Learning Rate: 0.000625\n",
      "Epoch 638, Loss: 0.05193723467323156, Learning Rate: 0.000625\n",
      "Epoch 639, Loss: 0.051937279590110075, Learning Rate: 0.000625\n",
      "Epoch 640, Loss: 0.05195347663648955, Learning Rate: 0.000625\n",
      "Epoch 641, Loss: 0.05197341049861722, Learning Rate: 0.000625\n",
      "Epoch 642, Loss: 0.052004284353090936, Learning Rate: 0.000625\n",
      "Epoch 643, Loss: 0.051849064177001046, Learning Rate: 0.000625\n",
      "Epoch 644, Loss: 0.05203776715842192, Learning Rate: 0.000625\n",
      "Epoch 645, Loss: 0.05193777064578466, Learning Rate: 0.000625\n",
      "Epoch 646, Loss: 0.052112580127265454, Learning Rate: 0.000625\n",
      "Epoch 647, Loss: 0.052002082001265916, Learning Rate: 0.000625\n",
      "Epoch 648, Loss: 0.05200050915321617, Learning Rate: 0.000625\n",
      "Epoch 649, Loss: 0.05203537231222906, Learning Rate: 0.000625\n",
      "Epoch 650, Loss: 0.052076672035551184, Learning Rate: 0.000625\n",
      "Epoch 651, Loss: 0.05211197956723641, Learning Rate: 0.000625\n",
      "Epoch 652, Loss: 0.05209843248756829, Learning Rate: 0.000625\n",
      "Epoch 653, Loss: 0.05213736036337803, Learning Rate: 0.000625\n",
      "Epoch 654, Loss: 0.05221749864901295, Learning Rate: 0.000625\n",
      "Epoch 655, Loss: 0.05220886486614347, Learning Rate: 0.000625\n",
      "Epoch 656, Loss: 0.052220074403115935, Learning Rate: 0.000625\n",
      "Epoch 657, Loss: 0.05224588237782298, Learning Rate: 0.000625\n",
      "Epoch 658, Loss: 0.05224247850795598, Learning Rate: 0.000625\n",
      "Epoch 659, Loss: 0.052208707957732525, Learning Rate: 0.000625\n",
      "Epoch 660, Loss: 0.0522073023850562, Learning Rate: 0.000625\n",
      "Epoch 661, Loss: 0.05220890609757831, Learning Rate: 0.000625\n",
      "Epoch 662, Loss: 0.05221218792517215, Learning Rate: 0.000625\n",
      "Epoch 663, Loss: 0.05223647511642461, Learning Rate: 0.000625\n",
      "Epoch 664, Loss: 0.05220944663701852, Learning Rate: 0.000625\n",
      "Epoch 665, Loss: 0.052204176414885076, Learning Rate: 0.000625\n",
      "Epoch 666, Loss: 0.052207119532319005, Learning Rate: 0.000625\n",
      "Epoch 667, Loss: 0.052186788445807465, Learning Rate: 0.000625\n",
      "Epoch 668, Loss: 0.05216552010631557, Learning Rate: 0.000625\n",
      "Epoch 669, Loss: 0.05215903344399238, Learning Rate: 0.000625\n",
      "Epoch 670, Loss: 0.05217902607043866, Learning Rate: 0.000625\n",
      "Epoch 671, Loss: 0.0521812476716607, Learning Rate: 0.000625\n",
      "Epoch 672, Loss: 0.05210988725784625, Learning Rate: 0.000625\n",
      "Epoch 673, Loss: 0.05210412943106453, Learning Rate: 0.000625\n",
      "Epoch 674, Loss: 0.05212494970702401, Learning Rate: 0.000625\n",
      "Epoch 675, Loss: 0.052155994510648084, Learning Rate: 0.000625\n",
      "Epoch 676, Loss: 0.05215765658430758, Learning Rate: 0.000625\n",
      "Epoch 677, Loss: 0.05217904654643559, Learning Rate: 0.000625\n",
      "Epoch 678, Loss: 0.05222222138737996, Learning Rate: 0.000625\n",
      "Epoch 679, Loss: 0.052223038421510125, Learning Rate: 0.000625\n",
      "Epoch 680, Loss: 0.05223470385412486, Learning Rate: 0.000625\n",
      "Epoch 681, Loss: 0.05223817738316072, Learning Rate: 0.000625\n",
      "Epoch 682, Loss: 0.05223207934406101, Learning Rate: 0.000625\n",
      "Epoch 683, Loss: 0.05223322751414711, Learning Rate: 0.000625\n",
      "Epoch 684, Loss: 0.052183409249797676, Learning Rate: 0.000625\n",
      "Epoch 685, Loss: 0.052135201296395615, Learning Rate: 0.000625\n",
      "Epoch 686, Loss: 0.052148700760407854, Learning Rate: 0.000625\n",
      "Epoch 687, Loss: 0.05217411097694887, Learning Rate: 0.000625\n",
      "Epoch 688, Loss: 0.05211470045446169, Learning Rate: 0.000625\n",
      "Epoch 689, Loss: 0.05210600610340177, Learning Rate: 0.000625\n",
      "Epoch 690, Loss: 0.05212798371875833, Learning Rate: 0.000625\n",
      "Epoch 691, Loss: 0.05214430916499859, Learning Rate: 0.000625\n",
      "Epoch 692, Loss: 0.05214222254461418, Learning Rate: 0.000625\n",
      "Epoch 693, Loss: 0.05214226300093945, Learning Rate: 0.000625\n",
      "Epoch 694, Loss: 0.05216628697287858, Learning Rate: 0.000625\n",
      "Epoch 695, Loss: 0.05216452441430521, Learning Rate: 0.000625\n",
      "Epoch 696, Loss: 0.05214358031729103, Learning Rate: 0.000625\n",
      "Epoch 697, Loss: 0.052099432737431284, Learning Rate: 0.000625\n",
      "Epoch 698, Loss: 0.052055021648450085, Learning Rate: 0.000625\n",
      "Epoch 699, Loss: 0.05206248940831122, Learning Rate: 0.000625\n",
      "Epoch 700, Loss: 0.052095239851893606, Learning Rate: 0.000625\n",
      "Epoch 701, Loss: 0.05213618350933293, Learning Rate: 0.000625\n",
      "Epoch 702, Loss: 0.05215331966110839, Learning Rate: 0.000625\n",
      "Epoch 703, Loss: 0.05217860145663069, Learning Rate: 0.000625\n",
      "Epoch 704, Loss: 0.05219314173693611, Learning Rate: 0.000625\n",
      "Epoch 705, Loss: 0.0522608468522953, Learning Rate: 0.000625\n",
      "Epoch 706, Loss: 0.052280840425299305, Learning Rate: 0.000625\n",
      "Epoch 707, Loss: 0.052340506054296666, Learning Rate: 0.000625\n",
      "Epoch 708, Loss: 0.052341327985926125, Learning Rate: 0.000625\n",
      "Epoch 709, Loss: 0.05238765224787627, Learning Rate: 0.000625\n",
      "Epoch 710, Loss: 0.05245142569580441, Learning Rate: 0.000625\n",
      "Epoch 711, Loss: 0.05247773820636831, Learning Rate: 0.000625\n",
      "Epoch 712, Loss: 0.05249487926160588, Learning Rate: 0.000625\n",
      "Epoch 713, Loss: 0.05277780357999553, Learning Rate: 0.000625\n",
      "Epoch 714, Loss: 0.05280901329193176, Learning Rate: 0.000625\n",
      "Epoch 715, Loss: 0.05281651924683043, Learning Rate: 0.000625\n",
      "Epoch 716, Loss: 0.052795643352556244, Learning Rate: 0.000625\n",
      "Epoch 717, Loss: 0.05281882622530304, Learning Rate: 0.000625\n",
      "Epoch 718, Loss: 0.05281206848968432, Learning Rate: 0.000625\n",
      "Epoch 719, Loss: 0.05287772280136781, Learning Rate: 0.000625\n",
      "Epoch 720, Loss: 0.052901084190219216, Learning Rate: 0.000625\n",
      "Epoch 721, Loss: 0.052942126256392164, Learning Rate: 0.000625\n",
      "Epoch 722, Loss: 0.05296321526828324, Learning Rate: 0.000625\n",
      "Epoch 723, Loss: 0.05298155916546654, Learning Rate: 0.000625\n",
      "Epoch 724, Loss: 0.052993717564011965, Learning Rate: 0.000625\n",
      "Epoch 725, Loss: 0.052972637451337554, Learning Rate: 0.000625\n",
      "Epoch 726, Loss: 0.052971259230182845, Learning Rate: 0.000625\n",
      "Epoch 727, Loss: 0.05301767471901937, Learning Rate: 0.000625\n",
      "Epoch 728, Loss: 0.05302916631793834, Learning Rate: 0.000625\n",
      "Epoch 729, Loss: 0.053013947824253414, Learning Rate: 0.000625\n",
      "Epoch 730, Loss: 0.05300439159642293, Learning Rate: 0.000625\n",
      "Epoch 731, Loss: 0.053027052013183903, Learning Rate: 0.000625\n",
      "Epoch 732, Loss: 0.0531453018820688, Learning Rate: 0.000625\n",
      "Epoch 733, Loss: 0.05313483046173386, Learning Rate: 0.000625\n",
      "Epoch 734, Loss: 0.05317323215623293, Learning Rate: 0.000625\n",
      "Epoch 735, Loss: 0.05319436602467118, Learning Rate: 0.000625\n",
      "Epoch 736, Loss: 0.053223021186888375, Learning Rate: 0.000625\n",
      "Epoch 737, Loss: 0.05325044076750265, Learning Rate: 0.000625\n",
      "Epoch 738, Loss: 0.05323121938961743, Learning Rate: 0.000625\n",
      "Epoch 739, Loss: 0.05329960160321606, Learning Rate: 0.000625\n",
      "Epoch 740, Loss: 0.053353647947689056, Learning Rate: 0.000625\n",
      "Epoch 741, Loss: 0.053316071138653, Learning Rate: 0.000625\n",
      "Epoch 742, Loss: 0.053384195445236315, Learning Rate: 0.000625\n",
      "Epoch 743, Loss: 0.05343833786001748, Learning Rate: 0.000625\n",
      "Epoch 744, Loss: 0.05345502949333516, Learning Rate: 0.000625\n",
      "Epoch 745, Loss: 0.05346747400975853, Learning Rate: 0.000625\n",
      "Epoch 746, Loss: 0.05346621094125139, Learning Rate: 0.000625\n",
      "Epoch 747, Loss: 0.053521042877333856, Learning Rate: 0.000625\n",
      "Epoch 748, Loss: 0.05355945694122348, Learning Rate: 0.000625\n",
      "Epoch 749, Loss: 0.05374718848629741, Learning Rate: 0.000625\n",
      "Epoch 750, Loss: 0.05379335876902397, Learning Rate: 0.000625\n",
      "Epoch 751, Loss: 0.05380739619531834, Learning Rate: 0.000625\n",
      "Epoch 752, Loss: 0.05383646332573202, Learning Rate: 0.000625\n",
      "Epoch 753, Loss: 0.0539093156292918, Learning Rate: 0.000625\n",
      "Epoch 754, Loss: 0.05393957646444249, Learning Rate: 0.000625\n",
      "Epoch 755, Loss: 0.05403316367873643, Learning Rate: 0.000625\n",
      "Epoch 756, Loss: 0.05411099373279828, Learning Rate: 0.000625\n",
      "Epoch 757, Loss: 0.05424379135220077, Learning Rate: 0.000625\n",
      "Epoch 758, Loss: 0.05423343902487128, Learning Rate: 0.000625\n",
      "Epoch 759, Loss: 0.0542223092314416, Learning Rate: 0.000625\n",
      "Epoch 760, Loss: 0.0544041181399779, Learning Rate: 0.000625\n",
      "Epoch 761, Loss: 0.0545634307287126, Learning Rate: 0.000625\n",
      "Epoch 762, Loss: 0.054499616303367485, Learning Rate: 0.000625\n",
      "Epoch 763, Loss: 0.054190054121486364, Learning Rate: 0.000625\n",
      "Epoch 764, Loss: 0.054099671428709105, Learning Rate: 0.000625\n",
      "Epoch 765, Loss: 0.053938922333668926, Learning Rate: 0.000625\n",
      "Epoch 766, Loss: 0.05366052883883658, Learning Rate: 0.000625\n",
      "Epoch 767, Loss: 0.05360360036097737, Learning Rate: 0.000625\n",
      "Epoch 768, Loss: 0.05364564829532277, Learning Rate: 0.000625\n",
      "Epoch 769, Loss: 0.053622596154268544, Learning Rate: 0.000625\n",
      "Epoch 770, Loss: 0.05357594802276764, Learning Rate: 0.000625\n",
      "Epoch 771, Loss: 0.05364583176883729, Learning Rate: 0.000625\n",
      "Epoch 772, Loss: 0.0538392022436693, Learning Rate: 0.000625\n",
      "Epoch 773, Loss: 0.05386292254081496, Learning Rate: 0.000625\n",
      "Epoch 774, Loss: 0.05385246144893374, Learning Rate: 0.000625\n",
      "Epoch 775, Loss: 0.05377195379898887, Learning Rate: 0.000625\n",
      "Epoch 776, Loss: 0.053829428587988434, Learning Rate: 0.000625\n",
      "Epoch 777, Loss: 0.05384086684211464, Learning Rate: 0.000625\n",
      "Epoch 778, Loss: 0.0538980863443415, Learning Rate: 0.000625\n",
      "Epoch 779, Loss: 0.05385346425666103, Learning Rate: 0.000625\n",
      "Epoch 780, Loss: 0.05395498813685169, Learning Rate: 0.000625\n",
      "Epoch 781, Loss: 0.053910162764013725, Learning Rate: 0.000625\n",
      "Epoch 782, Loss: 0.05381238762427735, Learning Rate: 0.000625\n",
      "Epoch 783, Loss: 0.053871199315608534, Learning Rate: 0.000625\n",
      "Epoch 784, Loss: 0.053904685320648515, Learning Rate: 0.000625\n",
      "Epoch 785, Loss: 0.05393096250579184, Learning Rate: 0.000625\n",
      "Epoch 786, Loss: 0.05391504988180089, Learning Rate: 0.000625\n",
      "Epoch 787, Loss: 0.05396183665671128, Learning Rate: 0.000625\n",
      "Epoch 788, Loss: 0.05397625250512944, Learning Rate: 0.000625\n",
      "Epoch 789, Loss: 0.05394295516703551, Learning Rate: 0.000625\n",
      "Epoch 790, Loss: 0.053971005403110305, Learning Rate: 0.000625\n",
      "Epoch 791, Loss: 0.05398598539108366, Learning Rate: 0.000625\n",
      "Epoch 792, Loss: 0.05398289071673303, Learning Rate: 0.000625\n",
      "Epoch 793, Loss: 0.054002909385284716, Learning Rate: 0.000625\n",
      "Epoch 794, Loss: 0.054000060190129505, Learning Rate: 0.000625\n",
      "Epoch 795, Loss: 0.054004008158350394, Learning Rate: 0.000625\n",
      "Epoch 796, Loss: 0.05400444201327108, Learning Rate: 0.000625\n",
      "Epoch 797, Loss: 0.054010917707838745, Learning Rate: 0.000625\n",
      "Epoch 798, Loss: 0.05402652963223327, Learning Rate: 0.000625\n",
      "Epoch 799, Loss: 0.05403430846500608, Learning Rate: 0.000625\n",
      "Epoch 800, Loss: 0.054018782304876646, Learning Rate: 0.000625\n",
      "Epoch 801, Loss: 0.05395756001740511, Learning Rate: 0.000625\n",
      "Epoch 802, Loss: 0.0540403764565643, Learning Rate: 0.000625\n",
      "Epoch 803, Loss: 0.054038080083738595, Learning Rate: 0.000625\n",
      "Epoch 804, Loss: 0.05405597895538324, Learning Rate: 0.000625\n",
      "Epoch 805, Loss: 0.05405971827817292, Learning Rate: 0.000625\n",
      "Epoch 806, Loss: 0.05407661011188867, Learning Rate: 0.000625\n",
      "Epoch 807, Loss: 0.0540388014207859, Learning Rate: 0.000625\n",
      "Epoch 808, Loss: 0.05405536485927627, Learning Rate: 0.000625\n",
      "Epoch 809, Loss: 0.054027501542765144, Learning Rate: 0.000625\n",
      "Epoch 810, Loss: 0.054019949930431556, Learning Rate: 0.000625\n",
      "Epoch 811, Loss: 0.05399421533118787, Learning Rate: 0.000625\n",
      "Epoch 812, Loss: 0.05407028306654529, Learning Rate: 0.000625\n",
      "Epoch 813, Loss: 0.05402222376879828, Learning Rate: 0.000625\n",
      "Epoch 814, Loss: 0.05399823387824019, Learning Rate: 0.000625\n",
      "Epoch 815, Loss: 0.05399639343883759, Learning Rate: 0.000625\n",
      "Epoch 816, Loss: 0.054002043192250174, Learning Rate: 0.000625\n",
      "Epoch 817, Loss: 0.053972152858009714, Learning Rate: 0.000625\n",
      "Epoch 818, Loss: 0.05397733944048875, Learning Rate: 0.000625\n",
      "Epoch 819, Loss: 0.05402850423079334, Learning Rate: 0.000625\n",
      "Epoch 820, Loss: 0.05405919656487777, Learning Rate: 0.000625\n",
      "Epoch 821, Loss: 0.05403641904582572, Learning Rate: 0.000625\n",
      "Epoch 822, Loss: 0.054073627068624494, Learning Rate: 0.000625\n",
      "Epoch 823, Loss: 0.054011172910994255, Learning Rate: 0.000625\n",
      "Epoch 824, Loss: 0.053917625922430344, Learning Rate: 0.000625\n",
      "Epoch 825, Loss: 0.05407540684525993, Learning Rate: 0.000625\n",
      "Epoch 826, Loss: 0.054131613067032555, Learning Rate: 0.000625\n",
      "Epoch 827, Loss: 0.054065390625582466, Learning Rate: 0.000625\n",
      "Epoch 828, Loss: 0.05414438167544031, Learning Rate: 0.000625\n",
      "Epoch 829, Loss: 0.05407359475025454, Learning Rate: 0.000625\n",
      "Epoch 830, Loss: 0.05410664385022399, Learning Rate: 0.000625\n",
      "Epoch 831, Loss: 0.05404841757770175, Learning Rate: 0.000625\n",
      "Epoch 832, Loss: 0.05394862857625175, Learning Rate: 0.000625\n",
      "Epoch 833, Loss: 0.05403624900378197, Learning Rate: 0.000625\n",
      "Epoch 834, Loss: 0.053927162152824504, Learning Rate: 0.000625\n",
      "Epoch 835, Loss: 0.05402372950808039, Learning Rate: 0.000625\n",
      "Epoch 836, Loss: 0.05391986720888505, Learning Rate: 0.000625\n",
      "Epoch 837, Loss: 0.05408615169200429, Learning Rate: 0.000625\n",
      "Epoch 838, Loss: 0.054173295516616496, Learning Rate: 0.000625\n",
      "Epoch 839, Loss: 0.05404662826288557, Learning Rate: 0.000625\n",
      "Epoch 840, Loss: 0.054103111726390284, Learning Rate: 0.000625\n",
      "Epoch 841, Loss: 0.053951137534410346, Learning Rate: 0.000625\n",
      "Epoch 842, Loss: 0.05394361313676688, Learning Rate: 0.000625\n",
      "Epoch 843, Loss: 0.054068037658305014, Learning Rate: 0.000625\n",
      "Epoch 844, Loss: 0.05394434694205865, Learning Rate: 0.000625\n",
      "Epoch 845, Loss: 0.054019211204620186, Learning Rate: 0.000625\n",
      "Epoch 846, Loss: 0.05405162372375396, Learning Rate: 0.000625\n",
      "Epoch 847, Loss: 0.05400490189869522, Learning Rate: 0.000625\n",
      "Epoch 848, Loss: 0.05415248185956637, Learning Rate: 0.000625\n",
      "Epoch 849, Loss: 0.054022977373250416, Learning Rate: 0.000625\n",
      "Epoch 850, Loss: 0.05404695109344907, Learning Rate: 0.000625\n",
      "Epoch 851, Loss: 0.05415698824711181, Learning Rate: 0.000625\n",
      "Epoch 852, Loss: 0.054066540207668934, Learning Rate: 0.000625\n",
      "Epoch 853, Loss: 0.05414504513092481, Learning Rate: 0.000625\n",
      "Epoch 854, Loss: 0.05406727873543956, Learning Rate: 0.000625\n",
      "Epoch 855, Loss: 0.05400604082014835, Learning Rate: 0.000625\n",
      "Epoch 856, Loss: 0.054186640796051434, Learning Rate: 0.000625\n",
      "Epoch 857, Loss: 0.05414706885923774, Learning Rate: 0.000625\n",
      "Epoch 858, Loss: 0.054001073808778034, Learning Rate: 0.000625\n",
      "Epoch 859, Loss: 0.0540998779971404, Learning Rate: 0.000625\n",
      "Epoch 860, Loss: 0.05418637283138785, Learning Rate: 0.000625\n",
      "Epoch 861, Loss: 0.05412838108824797, Learning Rate: 0.000625\n",
      "Epoch 862, Loss: 0.054125625546662635, Learning Rate: 0.000625\n",
      "Epoch 863, Loss: 0.054167548089591434, Learning Rate: 0.000625\n",
      "Epoch 864, Loss: 0.05409529701484757, Learning Rate: 0.000625\n",
      "Epoch 865, Loss: 0.054002911119997055, Learning Rate: 0.000625\n",
      "Epoch 866, Loss: 0.05394288245012402, Learning Rate: 0.000625\n",
      "Epoch 867, Loss: 0.0539656593729966, Learning Rate: 0.000625\n",
      "Epoch 868, Loss: 0.053979209663025496, Learning Rate: 0.000625\n",
      "Epoch 869, Loss: 0.05401037035375875, Learning Rate: 0.000625\n",
      "Epoch 870, Loss: 0.0540071111377602, Learning Rate: 0.000625\n",
      "Epoch 871, Loss: 0.053976307691334005, Learning Rate: 0.000625\n",
      "Epoch 872, Loss: 0.05393937174145765, Learning Rate: 0.000625\n",
      "Epoch 873, Loss: 0.05394155621609836, Learning Rate: 0.000625\n",
      "Epoch 874, Loss: 0.05392007858163237, Learning Rate: 0.000625\n",
      "Epoch 875, Loss: 0.05393882229858597, Learning Rate: 0.000625\n",
      "Epoch 876, Loss: 0.05390348401234848, Learning Rate: 0.000625\n",
      "Epoch 877, Loss: 0.05392197767701088, Learning Rate: 0.000625\n",
      "Epoch 878, Loss: 0.05393189162073199, Learning Rate: 0.000625\n",
      "Epoch 879, Loss: 0.054042616000195796, Learning Rate: 0.000625\n",
      "Epoch 880, Loss: 0.05392423661993588, Learning Rate: 0.000625\n",
      "Epoch 881, Loss: 0.05390316711044764, Learning Rate: 0.000625\n",
      "Epoch 882, Loss: 0.05392367969894638, Learning Rate: 0.000625\n",
      "Epoch 883, Loss: 0.05393082487207067, Learning Rate: 0.000625\n",
      "Epoch 884, Loss: 0.05391093449830031, Learning Rate: 0.000625\n",
      "Epoch 885, Loss: 0.05387563608869892, Learning Rate: 0.000625\n",
      "Epoch 886, Loss: 0.0538689440165413, Learning Rate: 0.000625\n",
      "Epoch 887, Loss: 0.05391095894160149, Learning Rate: 0.000625\n",
      "Epoch 888, Loss: 0.05386188670442653, Learning Rate: 0.000625\n",
      "Epoch 889, Loss: 0.05384131970408147, Learning Rate: 0.000625\n",
      "Epoch 890, Loss: 0.05385915077501471, Learning Rate: 0.000625\n",
      "Epoch 891, Loss: 0.053885252653076754, Learning Rate: 0.000625\n",
      "Epoch 892, Loss: 0.053870412485846876, Learning Rate: 0.000625\n",
      "Epoch 893, Loss: 0.05389497768513807, Learning Rate: 0.000625\n",
      "Epoch 894, Loss: 0.053858945126302474, Learning Rate: 0.000625\n",
      "Epoch 895, Loss: 0.05388996431581552, Learning Rate: 0.000625\n",
      "Epoch 896, Loss: 0.05385202819122318, Learning Rate: 0.000625\n",
      "Epoch 897, Loss: 0.053905457807811884, Learning Rate: 0.000625\n",
      "Epoch 898, Loss: 0.053878639888675625, Learning Rate: 0.000625\n",
      "Epoch 899, Loss: 0.053849992469649166, Learning Rate: 0.000625\n",
      "Epoch 900, Loss: 0.05385141038905984, Learning Rate: 0.000625\n",
      "Epoch 901, Loss: 0.05385373475622, Learning Rate: 0.000625\n",
      "Epoch 902, Loss: 0.05384534967516148, Learning Rate: 0.000625\n",
      "Epoch 903, Loss: 0.05386907379842241, Learning Rate: 0.000625\n",
      "Epoch 904, Loss: 0.05379603059856893, Learning Rate: 0.000625\n",
      "Epoch 905, Loss: 0.05387330182126111, Learning Rate: 0.000625\n",
      "Epoch 906, Loss: 0.05384374978188492, Learning Rate: 0.000625\n",
      "Epoch 907, Loss: 0.05380786524657809, Learning Rate: 0.000625\n",
      "Epoch 908, Loss: 0.05377569771094398, Learning Rate: 0.000625\n",
      "Epoch 909, Loss: 0.05377239301124662, Learning Rate: 0.000625\n",
      "Epoch 910, Loss: 0.05376073666095216, Learning Rate: 0.000625\n",
      "Epoch 911, Loss: 0.05375773892874782, Learning Rate: 0.000625\n",
      "Epoch 912, Loss: 0.053787138181061046, Learning Rate: 0.000625\n",
      "Epoch 913, Loss: 0.05375467937691105, Learning Rate: 0.000625\n",
      "Epoch 914, Loss: 0.05392064959485019, Learning Rate: 0.000625\n",
      "Epoch 915, Loss: 0.05374217365366633, Learning Rate: 0.000625\n",
      "Epoch 916, Loss: 0.053779977525636305, Learning Rate: 0.000625\n",
      "Epoch 917, Loss: 0.053831170371329465, Learning Rate: 0.000625\n",
      "Epoch 918, Loss: 0.053762327736119365, Learning Rate: 0.000625\n",
      "Epoch 919, Loss: 0.053741817091390164, Learning Rate: 0.000625\n",
      "Epoch 920, Loss: 0.05379672544531763, Learning Rate: 0.000625\n",
      "Epoch 921, Loss: 0.05373555808754429, Learning Rate: 0.000625\n",
      "Epoch 922, Loss: 0.05374792442838365, Learning Rate: 0.000625\n",
      "Epoch 923, Loss: 0.05373375408562957, Learning Rate: 0.000625\n",
      "Epoch 924, Loss: 0.05370643517629953, Learning Rate: 0.000625\n",
      "Epoch 925, Loss: 0.05371044746967975, Learning Rate: 0.000625\n",
      "Epoch 926, Loss: 0.05384768847208975, Learning Rate: 0.000625\n",
      "Epoch 927, Loss: 0.05370527434975529, Learning Rate: 0.000625\n",
      "Epoch 928, Loss: 0.053720730540184745, Learning Rate: 0.000625\n",
      "Epoch 929, Loss: 0.05374815173940396, Learning Rate: 0.000625\n",
      "Epoch 930, Loss: 0.05368930089956221, Learning Rate: 0.000625\n",
      "Epoch 931, Loss: 0.05379584460334702, Learning Rate: 0.000625\n",
      "Epoch 932, Loss: 0.0536401965098444, Learning Rate: 0.000625\n",
      "Epoch 933, Loss: 0.05366748273686195, Learning Rate: 0.000625\n",
      "Epoch 934, Loss: 0.053773534390567514, Learning Rate: 0.000625\n",
      "Epoch 935, Loss: 0.053619408033304214, Learning Rate: 0.000625\n",
      "Epoch 936, Loss: 0.05365005587584466, Learning Rate: 0.000625\n",
      "Epoch 937, Loss: 0.0537299655639241, Learning Rate: 0.000625\n",
      "Epoch 938, Loss: 0.053652204440090646, Learning Rate: 0.000625\n",
      "Epoch 939, Loss: 0.05372488823496058, Learning Rate: 0.000625\n",
      "Epoch 940, Loss: 0.05372903135045326, Learning Rate: 0.000625\n",
      "Epoch 941, Loss: 0.05369984243471394, Learning Rate: 0.000625\n",
      "Epoch 942, Loss: 0.05368334596490887, Learning Rate: 0.000625\n",
      "Epoch 943, Loss: 0.05374762231270051, Learning Rate: 0.000625\n",
      "Epoch 944, Loss: 0.053742315165054934, Learning Rate: 0.000625\n",
      "Epoch 945, Loss: 0.05380102577429367, Learning Rate: 0.000625\n",
      "Epoch 946, Loss: 0.053675777107483814, Learning Rate: 0.000625\n",
      "Epoch 947, Loss: 0.05369143793841833, Learning Rate: 0.000625\n",
      "Epoch 948, Loss: 0.05372354243125275, Learning Rate: 0.000625\n",
      "Epoch 949, Loss: 0.05382003001139015, Learning Rate: 0.000625\n",
      "Epoch 950, Loss: 0.053662322078306436, Learning Rate: 0.000625\n",
      "Epoch 951, Loss: 0.053805717875693534, Learning Rate: 0.000625\n",
      "Epoch 952, Loss: 0.053723690615671574, Learning Rate: 0.000625\n",
      "Epoch 953, Loss: 0.05366168974201162, Learning Rate: 0.000625\n",
      "Epoch 954, Loss: 0.0537009945861545, Learning Rate: 0.000625\n",
      "Epoch 955, Loss: 0.053654518376816826, Learning Rate: 0.000625\n",
      "Epoch 956, Loss: 0.05349544935228916, Learning Rate: 0.000625\n",
      "Epoch 957, Loss: 0.05355465849961416, Learning Rate: 0.000625\n",
      "Epoch 958, Loss: 0.05358439845690355, Learning Rate: 0.000625\n",
      "Epoch 959, Loss: 0.053486969461787194, Learning Rate: 0.000625\n",
      "Epoch 960, Loss: 0.05354061205979095, Learning Rate: 0.000625\n",
      "Epoch 961, Loss: 0.05357499530769072, Learning Rate: 0.000625\n",
      "Epoch 962, Loss: 0.05352037302277133, Learning Rate: 0.000625\n",
      "Epoch 963, Loss: 0.05346238537995718, Learning Rate: 0.000625\n",
      "Epoch 964, Loss: 0.05349439498484286, Learning Rate: 0.000625\n",
      "Epoch 965, Loss: 0.053432112750349516, Learning Rate: 0.000625\n",
      "Epoch 966, Loss: 0.05354205399098094, Learning Rate: 0.000625\n",
      "Epoch 967, Loss: 0.05338631190854716, Learning Rate: 0.000625\n",
      "Epoch 968, Loss: 0.053408017657382165, Learning Rate: 0.000625\n",
      "Epoch 969, Loss: 0.05343560475479244, Learning Rate: 0.000625\n",
      "Epoch 970, Loss: 0.053375753803855154, Learning Rate: 0.000625\n",
      "Epoch 971, Loss: 0.05342766642866722, Learning Rate: 0.000625\n",
      "Epoch 972, Loss: 0.05335746345006376, Learning Rate: 0.000625\n",
      "Epoch 973, Loss: 0.05342156114946943, Learning Rate: 0.000625\n",
      "Epoch 974, Loss: 0.053357605604236025, Learning Rate: 0.000625\n",
      "Epoch 975, Loss: 0.053394794702288625, Learning Rate: 0.000625\n",
      "Epoch 976, Loss: 0.05332085348567507, Learning Rate: 0.000625\n",
      "Epoch 977, Loss: 0.05332861428625228, Learning Rate: 0.000625\n",
      "Epoch 978, Loss: 0.05340067428688462, Learning Rate: 0.000625\n",
      "Epoch 979, Loss: 0.05329542541024897, Learning Rate: 0.000625\n",
      "Epoch 980, Loss: 0.05332998264889509, Learning Rate: 0.000625\n",
      "Epoch 981, Loss: 0.05330759548939298, Learning Rate: 0.000625\n",
      "Epoch 982, Loss: 0.05352016221559932, Learning Rate: 0.000625\n",
      "Epoch 983, Loss: 0.05331314837654484, Learning Rate: 0.000625\n",
      "Epoch 984, Loss: 0.053513401856991605, Learning Rate: 0.000625\n",
      "Epoch 985, Loss: 0.05343404603615824, Learning Rate: 0.000625\n",
      "Epoch 986, Loss: 0.053528082614032434, Learning Rate: 0.000625\n",
      "Epoch 987, Loss: 0.05345953464434935, Learning Rate: 0.000625\n",
      "Epoch 988, Loss: 0.05351277880494139, Learning Rate: 0.000625\n",
      "Epoch 989, Loss: 0.05349581571149069, Learning Rate: 0.000625\n",
      "Epoch 990, Loss: 0.05346617536746212, Learning Rate: 0.000625\n",
      "Epoch 991, Loss: 0.05346548888399007, Learning Rate: 0.000625\n",
      "Epoch 992, Loss: 0.053429058162740975, Learning Rate: 0.000625\n",
      "Epoch 993, Loss: 0.053461821265953766, Learning Rate: 0.000625\n",
      "Epoch 994, Loss: 0.05352068812609696, Learning Rate: 0.000625\n",
      "Epoch 995, Loss: 0.05347152568927525, Learning Rate: 0.000625\n",
      "Epoch 996, Loss: 0.05343552769390132, Learning Rate: 0.000625\n",
      "Epoch 997, Loss: 0.05348008867680278, Learning Rate: 0.000625\n",
      "Epoch 998, Loss: 0.05342861536877973, Learning Rate: 0.000625\n",
      "Epoch 999, Loss: 0.05347035604003455, Learning Rate: 0.000625\n"
     ]
    }
   ],
   "source": [
    "initial_learning_rate = 0.01\n",
    "epochs = 1000\n",
    "batch_size = 64\n",
    "patience = 10\n",
    "factor = 0.5\n",
    "threshold = 1e-4\n",
    "\n",
    "w1, b1, w2, b2 = train(x_train, y_train, initial_learning_rate, epochs, batch_size, patience, factor, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(mndata.test_images, w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5826"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred == mndata.test_labels) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
